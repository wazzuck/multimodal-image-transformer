Preprint

D EMYSTIFYING CLIP DATA
Hu Xu1 Saining Xie2 Xiaoqing Ellen Tan1 Po-Yao Huang1 Russell Howes1 Vasu Sharma1
Shang-Wen Li1
Gargi Ghosh1
Luke Zettlemoyer1,3
Christoph Feichtenhofer1
1
2
3
FAIR, Meta AI
New York University
University of Washington

arXiv:2309.16671v5 [cs.CV] 28 Dec 2024

A BSTRACT
Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced
research and applications in computer vision, fueling modern recognition systems
and generative models. We believe that the main ingredient to the success of
CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has
been collected, leading to works that aim to reproduce CLIP’s data by filtering
with its model parameters. In this work, we intend to reveal CLIP’s data curation approach and in our pursuit of making it open to the community introduce
Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a
raw data pool and metadata (derived from CLIP’s concepts) and yields a balanced
subset over the metadata distribution. Our experimental study rigorously isolates
the model and training settings, concentrating solely on data. MetaCLIP applied
to CommonCrawl with 400M image-text data pairs outperforms CLIP’s data on
multiple standard benchmarks. In zero-shot ImageNet classification, MetaCLIP
achieves 70.8% accuracy, surpassing CLIP’s 68.3% on ViT-B models. Scaling
to 1B data, while maintaining the same training budget, attains 72.4%. Our observations hold across various model sizes, exemplified by ViT-bigG producing
82.1%. Curation code and training data distribution over metadata is available at
https://github.com/facebookresearch/MetaCLIP.

1

I NTRODUCTION

Deep learning has revolutionized the field of artificial intelligence, and pre-trained models have
played a pivotal role in democratizing access to cutting-edge AI capabilities. However, the training
data used to create these models is often concealed from the public eye, shrouded in secrecy.
The increasing availability of pre-trained models for public use contrasts sharply with the lack of
transparency regarding their training data. Further, proprietary concerns, such as copyright issues,
often limit access to the original data sources. Consequently, the need to explore novel approaches
for curating high-quality training data that can be shared openly arises.
In the vision-language domain, the dominant model and learning approach is Contrastive LanguageImage Pre-training (CLIP) (Radford et al., 2021), a simple technique to learn from image-text
pairs. We believe that the secret to the dominance of CLIP models is attributed to its high-quality
WIT400M dataset which is curated from the web. Despite its popularity, the specifics of CLIP’s
curation process have remained a mystery, captivating the research community for years.
Follow-up works (Schuhmann et al., 2022; 2021) have attempted to replicate CLIP’s data, but with
a notable difference in their curation method. While CLIP generates data based on its unknown data
source and curation methodology, these approaches remove noise by applying the CLIP model as a
hard blackbox filter which in turn is a form of distilling WIT400M information captured in CLIP.
The advantages of CLIP’s curation are apparent. First, it starts from scratch, avoiding the introduction of biases through filters. Second, CLIP’s curation process balances the data distribution over
metadata, maximizing signal preservation while mitigating, rather than removing, noise in the data1 .
Such distribution lays the groundwork for task-agnostic data, a crucial part of foundation models.
1
For example, a filter on digits can remove noise from date or id strings but remove signal for tasks that
involve OCR (e.g., MNIST), or a filter removing text with less than 5 characters can remove signal “dog”.

1

Preprint

Our algorithm takes a raw data pool
D and metadata M (derived from
CLIP’s queries or visual concepts) and
yields a balanced subset D∗ over M:
D∗ ← f (D; M). Our approach, named
Metadata-Curated Language-Image Pretraining (MetaCLIP), marks a significant
step towards making the curation process
more transparent and accessible.

0.70
MetaCLIP(400M)

0.65

MetaCLIP w/o bal.(400M)

0.60

ImageNet Zero-shot Acc.

In this paper, we attempt to reveal CLIP’s
method around training data curation. We
present an empirical study on data curation, with frozen model architecture and
training schedule. We focus solely on the
impact of training data, excluding other
factors that could confound the results. We
make several observations for good data
quality and present a simple algorithm to
make CLIP’s curation more transparent.
Consequently, we shed light on both the
curation process and the resulting training
data distribution. Our algorithm enables
easy adaptation to different data pools,
allowing parties to fully own their data
pipeline without relying on blackbox filters from external providers.

Raw English(400M)
0.55

Raw(1.1B)

0.50
0.45
0.40
CLIP(400M)
LAION(407M)
Raw(1.1B)
Raw English(400M)
MetaCLIP w/o bal.(400M)
MetaCLIP(400M)

0.35
0.30

0

50000

100000 150000 200000 250000 300000 350000 400000

Training Steps

Figure 1: ViT-B/32 on ImageNet zero-shot classification with fixed training steps (12.8B seen pairs and
training/validation data has been de-duplicated). Raw:
raw CommonCrawl (CC) distribution; Raw English:
English only CC; MetaCLIP w/o bal.: curated (substring matched) data pool from CC; MetaCLIP: curated and balanced metadata distribution. Metadata
curation boosts performance significantly and balancing is equally important. Our MetaCLIP data significantly outperforms CLIP’s WIT400M and LAION
data(Schuhmann et al., 2021).

MetaCLIP applied to CommonCrawl (CC)
with 400M data points outperforms CLIP
on multiple standard benchmarks. In
terms of zero-shot ImageNet classification, using ViT (Dosovitskiy et al., 2020)
models of various sizes. Our MetaCLIP
achieves 70.8% vs CLIP’s 68.3% on ViT-B and 76.2% vs 75.5% on ViT-L. Scaling to 2.5B data,
with the same training budget and similar distribution boosts this to unprecedented accuracy of
79.2% for ViT-L , 80.5% for ViT-H and 82.1% for ViT-bigG in the vanilla training setting (not using
any external data, models, or longer training).
In Fig.1, we show the impact of metadata curation on ImageNet validation plotted over training
steps. First, we are training on Raw English data from the web (400M image-text pairs, 57.4% accuracy), after applying Language IDentification (LID) to the random Raw set (∼1.1B pairs, 54.1%).
Using metadata to curate the training set (MetaCLIP 400M w/o bal, 60.8%) performs significantly
better than these baselines, and using balancing significantly increases accuracy further (MetaCLIP, 65.5%), outperforming similar datasets, WIT400M from CLIP, 63.4% and LAION 400M,
60.0%(Schuhmann et al., 2021).

2

R ELATED W ORK

The training data of CLIP differs significantly from a traditional supervised dataset (Gadre et al.,
2023) in various aspects. Firstly, it involves large-scale training with mixed-quality image-text pairs
rather than categorized images with human annotated labels, as commonly seen in classification
datasets. Secondly, CLIP’s pre-training is the initial stage of training, assuming no access to previously trained models.
Data Pruning on Established Datasets. Current research on data algorithms primarily revolves
around data pruning techniques applied to well-established datasets using pre-trained models
(Sorscher et al., 2022; Abbas et al., 2023). These approaches, such as coreset selection techniques
(Har-Peled & Mazumdar, 2004; Feldman et al., 2011; Bachem et al., 2015; Mirzasoleiman et al.,
2

Preprint

2020; Toneva et al., 2018), aim to select a subset of data that yields similar performance to training
on the entire dataset. Post-hoc data pruning with model filters has limited utility, if the model is used
as a black-box filter that forbids to control biases or improve the filter quality.
Handling Noisy Internet Data. Addressing noisy data from the Internet is a significant challenge,
and existing approaches often heavily rely on human-designed filter systems. Classical methods
involve dataset cleaning and outlier removal (Jiang et al., 2001; Yu et al., 2002) to discard samples
that may introduce undesirable biases to models.
Replicating CLIP’s Training Data. Recent efforts, such as LAION (Schuhmann et al., 2021;
2022) and concurrent work DataComp (Gadre et al., 2023), attempt to replicate CLIP’s training
data. However, they adopt fundamentally different strategies for several reasons. First, the data used
in these approaches are post-hoc, filtered, by vanilla CLIP as a teacher model. Second, the curation
process in these methods relies on a labor-intensive pipeline of filters, making it challenging to
comprehend the resulting data distribution from the raw Internet (refer to the unknown biases of
using CLIP filter in (Schuhmann et al., 2022)). Thirdly, the goal is to match the quantity of CLIP’s
target data size rather than the data distribution itself, which may lead to an underestimation of the
data pool size needed to obtain sufficient quality data. Consequently, the performance on the 400M
scale is sub-optimal, with LAION400M only achieving 72.77% (Schuhmann et al., 2021) accuracy
on ViT-L/14 on ImageNet, whereas vanilla CLIP obtains 75.5%.
Importance of Understanding CLIP’s Data Curation. The observations made in these studies
underscore the critical importance of understanding how OpenAI CLIP curates its data in the first
place. A comprehensive understanding of the curation process can shed light on the factors that
contribute to its success, allowing researchers to devise more effective and efficient algorithms for
future vision-language pre-training endeavors.

3

M ETACLIP

The original paper (Radford et al., 2021) only provides limited details about how CLIP curates
its data. Since important design choices for a direct reproduction are missing, we will clarify our
choices in this section. Our goal is to uncover CLIP’s data curation process, which involves preserving signal in the data while minimizing noise. In this section, we will explain the principles we have
adopted to achieve this, which may differ from CLIP’s as these are not known publicly.
CLIP’s WIT400M is curated with an information retrieval method, quoting (Radford et al., 2021):

“text)To pairs
address this, we constructed a new dataset of 400 million (image,
collected from a variety of publicly available sources on the Internet. To attempt to cover as broad a set of visual concepts as possible,
we search for (image, text) pairs as part of the construction process whose
text includes one of a set of 500,000 queries We approximately class balance the results by including up to 20,000 (image, text) pairs per query.

”
We rigorously adhere to this description and provide detailed insights into the construction process
of CLIP’s metadata (in §3.1)2 , sub-string matching (in §3.2), inverted indexing (in §3.3), as well as
query and balancing (in §3.4).
3.1

M ETADATA CONSTRUCTION : M = {entry}

We start by re-building CLIP’s 500,000-query metadata, citing Radford et al. (2021):
2
We generalize the term queries (used by CLIP) as entries in metadata because metadata describe training
data and our algorithm does not require search on inverted index yet have similar effects.

3

Preprint

“sionTheof base
query list is all words occurring at least 100 times in the English verWikipedia. This is augmented with bi-grams with high pointwise mutual
information as well as the names of all Wikipedia articles above a certain search
volume. Finally all WordNet synsets not already in the query list are added.

”
The metadata (‘queries’ or ‘entries’) consists of four components: (1) all synsets of WordNet, (2)
uni-grams from the English version of Wikipedia occurring at least 100 times, (3) bi-grams with high
pointwise mutual information, and (4) titles of Wikipedia articles above a certain search volume. We
rebuild these components from WordNet and Wikipedia and summarize the statistics in Table 13 . We
estimate the thresholds for components (3) and (4) as in the 3rd column of Table 1, by first choosing
a point-wise mutual information threshold of 30 that meets the budget of 100k entries for bi-grams
and then fill the rest of the entries with Wikipedia titles.
Source
WordNet synsets
Wiki uni-gram
Wiki bi-gram
Wiki titles

# of Entries
86,654
251,465
100,646
61,235

Desc. of Threshold
N/A
Count
Pointwise Mutual Info.(PMI)
View Frequency

Threshold
[ALL] (follow CLIP)
100 (follow CLIP)
30 (estimated)
70 (estimated)

Table 1: Composition of MetaCLIP Metadata.

3.2

S UB - STRING M ATCHING : text → entry

After constructing the metadata, CLIP’s curation aligns a pool of image-text pairs with metadata
entries through sub-string matching. This process identifies texts that contain any of the metadata
entries, effectively associating unstructured texts with structured metadata entries. The sub-string
matching step retains only high-quality matching texts, automatically filtering out various types of
noises that a typical filter system would consider on a case-by-case basis.
Such alignment is referred to as sub-string matching in Radford et al. (2021):

“whileWe most
also restrict this step in CLIP to text-only querying for sub-string matches
webly supervised work uses standard image search engines ...
”
Image-Text Pair Pool We start by estimating the pool size used by CLIP’s curation. CLIP’s data
source is unknown to us (“a variety of publicly available sources” in Radford et al. (2021)). We
adopt CommonCrawl (CC)4 as the source to build such a pool and re-apply sub-string matching
to this source. We ended with a pool of 1.6B image-text pairs (5.6B counts of sub-string matches).
Note that one text can have multiple matches of entries and we have 3.5 matches per text on average.
As a result, sub-string matching builds the mapping txt → entry. This step has two outcomes: (1)
low-quality text is dropped; (2) unstructured text now has a structured association with metadata.
For all English text, ∼50% image-text pairs are kept in this stage. Similar to CiT (Xu et al., 2023),
this approach looks for quality matches and automatically gets rid of some type of noise (such as
date strings) that a typical filter system would remove consider case-by-case (e.g., regular expression
on dates, ids etc.).
3
Note that we cannot find Wikipedia’s search volume for titles of Wikipedia (4). Instead, we use volumes
of Pageviews on Wiki articles. We randomly selected 26 days’ Pageviews from Apr. 2018 to Sep. 2022.
4
https://commoncrawl.org

4

Preprint

Metadata Subset
Full
Counts = 0
Counts > 20000

# of Entries
500K
114K
16K

# of Counts
5.6B
0
5.35B

Table 2: Summary of counts for entries.
Entry
of
the
photo
on
at

Counts
120M
87M
54M
45M
38M

Entry
in
The
a
by
Black

Counts
107M
67M
50M
43M
33M

Entry
and
with
image
2
3

Counts
100M
67M
48M
43M
30M

Entry
for
to
1
Image
A

Counts
89M
61M
47M
39M
29M

Table 3: Top-20 entries with counts.

3.3

I NVERTED I NDEXING : entry → text

Following sub-string matching, CLIP builds an inverted index of the data pool. All texts associated with each metadata entry are aggregated into lists, creating a mapping from each entry to the
corresponding texts, entry → text.
As an analysis, we count the number of matches for each entry and summarize that in Table 2. The
counts exhibit a long-tailed distribution. Out of the 500k entries, 114k entries have no matches. This
signifies the importance of knowing the training data distribution since it is very likely the training
data does not have certain visual concepts. We observed that only 16k entries had counts higher
than 20k, accounting for only 3.2% (16k/500k) of the entries, but their counts made up 94.5%
(5.35B/5.6B) of the total counts of all entries.
Top Entries. We show the top entries of the matching in Table 3. Interestingly, many of these
are stopwords, which don’t carry specific meaning but can enhance the overall text quality (e.g., by
generating grammatically correct sentences rather than just keyword lists). It’s important to note
that although sub-string matching aims to select only high-quality texts, there are instances where
common entries may still include irrelevant texts. For instance, the entry "photo" could match with
the popular but unhelpful term "untitled photo". These noise-related issues can be addressed in the
subsequent stage of processing.
3.4

Q UERY AND BALANCING WITH t ≤20 K

The key secret behind OpenAI CLIP’s curation is to balance the counts of matched entries. For
each metadata entry, the associated list of texts (or image-text pairs) is sub-sampled, ensuring that
the resulting data distribution is more balanced. This step aims to mitigate noise and diversify the
distribution of data points, making the data more task-agnostic as foundation data for pre-training.
The magic number t = 20k is a threshold used to limit the number of texts/pairs for each entry.
Entries with fewer than t pairs (tail entries) retain all associated pairs, while entries with more than t
pairs (head entries) are sub-sampled to t pairs. The selection is based on the density of information in
texts; texts with more matched entries have a higher chance of being curated (recall that the average
is 3.5 matches per text).
To study the effect of the magic number t = 20k, we plot the cumulative sum of counts for entries
sorted by counts from tail to head in Fig. 2. Interestingly, the value of t = 20k seemingly represents
the transition from tail to head entries, when the head entries start exhibiting an exponential growth
rate. By applying a max count of t, the growth rate of total counts (i.e., the scale of resulting data
points) is reduced to linear. This significantly flattens (and balances) the training data distribution.
We further study the optimality of t = 20k for the 400M data scale in our experiments.
In summary, balancing yields three interesting outcomes:
5

Preprint

Pool (1.6B)

t=20k (400M)

Figure 2: Cumulative sum of counts on entries from tail to head on a data pool with 1.6B image-text
pairs (5.6B match counts). (1) raw/unbalanced cumulative counts, t = ∞; (2) balanced cumulative
counts after applying t = 20k. The limit t defines the transition of tail/head entries.

(i) It reduces dominance and noise from head entries, like common web terms. E.g., out of 400M
pairs, only 20k texts containing “photo” are kept (while there are 54M “photo” instances in the
pool).
(ii) It diversifies the data distribution and balances tail/head entries, leading to a more task-agnostic
foundation.
(iii) Sampling for each entry ensures that data points with more matched entries or denser information are prioritized for curation.
Discussion. CLIP employs a pure NLP-based approach, requiring no access to ML models and
minimizing explicit/implicit priors from humans. The metadata plays a central role in mitigating
noise and preserving signal in the data distribution. The balancing step effectively flattens the data
distribution, diversifying the data and making it more suitable as foundation data for pre-training
tasks. We analyze the effects of balancing in Appendix A.3.
3.5

A SIMPLE A LGORITHM FOR C URATION

This section presents an algorithm that formalizes the curation process described earlier. The algorithm aims to improve scalability and reduce space complexity for operations across data points,
such as inverted indexing and sub-sampling. Instead of building inverted indexes, the algorithm only
maintains total counts for each entry.
We assume that CLIP curation constructs an inverted index that maps entries to documents (imagetext pairs) to enable efficient search for each entry (“we search for (image-text) pairs” in Radford
et al. (2021)). In contrast, our algorithm approaches the balancing process through independent
sampling. This avoids the need to build an inverted index that could potentially store hundreds of
millions of concrete pairs for popular entries, thereby improving efficiency and scalability.
Our algorithm takes three inputs: metadata M, a data pool D, and a hyper-parameter t. It aims to
find a subset D∗ with a balanced distribution over M, denoted as D∗ ← f (D; M, t). The algorithm
consists of two parts, each corresponding to a specific stage of the curation process.
We provide the Python pseudo-code in Algorithm 1.
6

Preprint

Algorithm 1: Pseudo-code of Curation Algorithm in Python style (see Sec. A.10 for samples).
# D: raw image-text pairs;
# M: metadata;
# t: max matches per entry in metadata;
# D_star: curated image-text pairs;
D_star = []
# Part 1: sub-string matching: store entry indexes in text.matched_entry_ids and
output counts per entry in entry_count.
entry_count = substr_matching(D, M)
# Part 2: balancing via indepenent sampling
entry_count[entry_count < t] = t
entry_prob = t / entry_count
for image, text in D:
for entry_id in text.matched_entry_ids:
if random.random() < entry_prob[entry_id]:
D_star.append((image, text))
break

Part 1: Entry Counts from Sub-string Matching. This corresponds to Sec. 3.2. The
substr_matching function outputs the total counts of matches per entry, entry_count,
represented as a NumPy array indexed by entry_id. Each text is associated with
matched_entry_ids that contains a list of matched entries.
Part 2: Balancing via Independent Sampling. This part corresponds to Sec.3.3 and Sec.3.4
and focuses on balancing counts on entries. Instead of building an expensive inverted index with
associated lists of texts for each entry, we sample each data point independently.
We first compute the probability of sampling each entry, entry_prob, where tail entries
(entry_count < t) have a probability equal to 1, and head entries have a probability less than
1. We iterate through all image-text pairs and sample/curate each pair. When an image-text pair has
a matched entry sampled/selected, we include that pair in D∗ .
This procedure is equivalent to CLIP’s curation, because if one image-text pair has one or more
matched entries, the chance of that pair being selected is determined by the probability of sampling
for each individual entry: t/entry_count[entry_id]. As long as one entry selects that pair,
it will be kept in D∗ . Our independent sampling approach allows us to scale balancing for each data
point independently and reduces the global operation to counting the total matches for each entry.
We demonstrate case studies in experiments on (1) scaling curation in a data pipeline and (2) online
balancing in data loader.

4

E XPERIMENTS

Data Pools.

We collect two pools of data:

Pool 1 contains 1.6 billion image-text pairs with a total of 5.6 billion counts of matches. This pool
was used to estimate a target of 400M image-text pairs, collected from 15 snapshots of CommonCrawl (CC) from January 2021 to January 2023.
Pool 2 aims to scale curation in our data pipeline. We parsed all 90 CC snapshots from 2013 to
April 2023, using our algorithm (see §A.2 for details on the curation pipeline) to curate from a pool
of 10.7B matched image-text pairs that are originally from a large set of URL-text pairs, which
have undergone de-duplication, English Language IDentification (LID) and sub-string matching.
However, we only perform (expensive) image downloading, storing, and transferring for data points
that are distribution-calibrated and selected by our algorithm.
For balancing we consider 2 scenarios on this data: (i) t = 170k, which is resulting in 2.5B imagetext pairs. This t = 170k configuration has tail counts amounting to 6% of the total counts, the same
tail/head ratio that the 400M Pool 1 data has, produced by applying t = 20k on the 1.6B Pool 1
data. (ii) The t = 20k threshold applied to Pool 2 which results in 1B image-text pairs and compared
to the 400M set from Pool 1 only increases tail metadata matches (head counts are capped at 20k).
7

SST2

HatefulMemes

CLEVR

Kinetics700

UCF101

PCAM

Country211

GTSRB

KITTI

RESISC45

EuroSAT

STL-10

FER-2013

MNIST

Caltech-101

Flowers

Pets

Aircraft

DTD

Cars

SUN397

CUB

CIFAR100

CIFAR10

ImageNet

Average

ViT-B/32
CLIP, our eval.
56.6
OpenCLIP, our eval. 57.6
MetaCLIP
58.2
ViT-B/16
CLIP, our eval.
59.6
OpenCLIP, our eval. 60.4
MetaCLIP
61.1
ViT-L/14
CLIP, our eval.
65.7
OpenCLIP, our eval. 64.5
MetaCLIP
67.1

Food-101

Preprint

63.4
62.9
65.5

83.7 89.8 65.1 53.7 62.0 59.7 19.6 44.0 87.2 87.4 66.9 48.2 46.6 97.1 44.9 61.0 32.6 28.7 17.2 62.5 63.9 48.0 23.6 56.4 58.6
80.7 90.7 70.6 61.2 66.4 79.2 16.7 54.5 86.5 90.7 66.1 37.4 48.2 95.6 52.2 58.0 42.0 38.0 14.8 50.1 63.0 42.8 22.5 53.3 52.3
80.6 91.3 70.2 63.4 63.0 70.7 26.8 52.8 88.7 91.9 68.5 41.5 35.9 95.4 52.6 64.2 35.8 30.7 17.2 55.5 66.1 45.4 30.6 56.4 53.4

68.3
67.0
70.8

88.8 90.8 68.2 55.6 64.0 64.6 24.0 45.1 88.9 89.1 69.4 51.8 53.0 98.2 54.8 65.5 43.3 21.7 22.8 56.3 68.5 52.3 25.5 58.7 60.5
85.8 91.7 71.4 65.3 69.2 83.6 17.4 51.0 89.2 90.8 66.5 66.3 46.1 97.0 52.2 65.7 43.5 23.7 18.1 51.7 67.0 46.2 33.9 54.5 54.4
86.8 90.1 66.5 70.8 66.6 74.1 27.9 55.9 90.4 93.8 72.3 47.8 44.6 97.2 55.4 68.8 43.8 33.4 22.6 52.9 68.0 49.5 22.8 54.8 60.6

75.5
72.7
76.2

93.0 95.6
90.0 94.7
90.7 95.5

78.3 63.3 66.8 77.8 31.3 55.3 93.6 93.3 79.3 76.4 56.9 99.4 61.9 70.9 50.6 19.2 31.9 50.1 75.7 60.2 22.3 59.7 68.9
78.0 73.9 72.4 89.5 24.7 60.2 91.6 93.6 73.0 76.1 54.3 98.1 63.9 69.6 49.9 16.0 23.0 51.7 71.5 51.6 25.4 55.3 56.0
77.4 75.9 70.5 84.7 40.4 62.0 93.7 94.4 76.4 61.7 46.5 99.3 59.7 71.9 47.5 29.9 30.9 70.1 75.5 57.1 35.1 56.6 65.6

SST2

HatefulMemes

CLEVR

Kinetics700

UCF101

PCAM

Country211

KITTI

GTSRB

RESISC45

EuroSAT

STL-10

FER-2013

MNIST

Flowers

Caltech-101

Pets

DTD

Aircraft

Cars

SUN397

CUB

CIFAR100

CIFAR10

Food-101

Average

ImageNet

Table 4: MetaCLIP-400M vs. CLIP (WIT400M data) and OpenCLIP (LAION-400M
data(Schuhmann et al., 2021)). We use 3 different model scales (ViT-B/32 and -B/16 and -L/14)
and an identical training setup as CLIP.

ViT-B/32
MetaCLIP(400M) 58.2
60.3
MetaCLIP(1B)
MetaCLIP(2.5B) 59.8

65.5 80.6 91.3
67.3 81.9 95.2
67.6 82.6 95.2

70.2 63.4 63.0 70.7 26.8 52.8 88.7 91.9 68.5 41.5 35.9 95.4 52.6 64.2 35.8 30.7 17.2 55.5 66.1 45.4 30.6 56.4 53.4
76.7 71.4 65.9 73.0 31.4 58.9 89.5 92.5 72.6 35.4 45.8 96.3 50.4 64.6 40.7 32.0 17.0 64.2 70.3 47.8 14.6 54.9 56.8
77.7 67.8 66.8 77.2 26.9 58.9 90.9 92.5 69.7 42.7 48.3 96.3 49.9 66.5 39.2 29.3 17.7 50.0 68.0 47.6 19.4 53.5 53.1

ViT-B/16
MetaCLIP(400M) 61.1
MetaCLIP(1B)
63.2
MetaCLIP(2.5B) 63.5

70.8 86.8 90.1
72.4 88.1 94.8
72.1 88.3 95.7

66.5 70.8 66.6 74.1 27.9 55.9 90.4 93.8 72.3 47.8 44.6 97.2 55.4 68.8 43.8 33.4 22.6 52.9 68.0 49.5 22.8 54.8 60.6
78.2 77.5 66.4 79.3 38.0 57.7 92.3 93.6 75.1 36.4 47.8 98.0 50.5 70.1 49.5 36.6 21.6 53.7 74.1 52.7 21.6 56.8 61.6
79.0 71.4 68.5 82.9 30.3 62.1 91.7 93.3 73.9 66.1 47.0 98.4 51.1 71.1 46.6 16.6 22.7 50.5 73.0 52.5 30.8 57.4 59.0

ViT-L/14
MetaCLIP(400M) 67.1
MetaCLIP(1B)
70.2
MetaCLIP(2.5B) 69.8

76.2 90.7 95.5 77.4 75.9 70.5 84.7 40.4 62.0 93.7 94.4 76.4 61.7 46.5 99.3 59.7 71.9 47.5 29.9 30.9 70.1 75.5 57.1 35.1 56.6 65.6
79.0 92.9 96.8 84.9 83.1 72.8 86.5 48.9 65.9 95.3 94.8 84.7 53.8 54.1 99.3 70.0 73.8 58.7 36.3 32.2 70.4 81.4 61.6 21.1 61.2 66.1
79.2 93.4 97.6 84.2 80.1 73.8 88.7 44.6 68.1 94.7 95.4 81.8 64.4 55.1 99.3 59.2 74.6 56.3 29.7 34.0 67.3 81.6 62.0 25.9 58.0 66.7

ViT-H/14
MetaCLIP(2.5B)

72.4

80.5

94.2

98.0

86.4

83.4

74.1

90.0

50.2

72.4

95.4

95.6

85.1

72.7

55.2

99.4

66.3

74.6

62.5

38.2

37.2

65.8

82.2

64.1

30.1

59.3

69.2

ViT-bigG/14
MetaCLIP(2.5B)

73.2

82.1

94.9

98.5

88.6

84.0

74.7

90.9

52.7

72.6

96.1

95.7

89.5

78.1

56.7

99.5

73.7

75.5

61.7

31.0

41.5

65.6

85.6

65.8

24.3

58.8

65.3

Table 5: Scaling MetaCLIP from 400M (t=20k) to 1B (t=20k) and 2.5B (t=170k) training data.
Training Setup We strictly follow the CLIP training setup, using V100 32GB GPUs and an equivalent global batch size of 32,768. For ViT-B/32 and ViT-B/16, we use 64 GPUs with a per GPU batch
size of 512 and for ViT-L/14 we use 128 GPUs with a 256 per GPU batch size. It takes 4 days to
train ViT-B/32 and a month to train ViT-L/14. We use 256 A100 80GB GPUs to train ViT-H/14 and
ViT-bigG/14 model for 1 week and 2 months, respectively. We train in all experiments for the same
number of iterations that correspond to 12.8B seen image-text pairs during training (32 epochs for
400M). We pre-process with face-blurring.
4.1

R ESULTS

Zero-shot Image Classification. We follow the standard evaluation benchmark and made sure all
prompts and class names were the same as those used by CLIP Radford et al. (2021). We also reevaluated OpenAI/OpenCLIP’s checkpoints to avoid differences caused by benchmark data copies.
The results are shown in Tab 4. The standard deviation of training multiple seeds is relatively small
with ± 0.1% for ImageNet on ViT-B/32.
In Table 4, we observe that MetaCLIP outperforms OpenAI CLIP on ImageNet and average accuracy
across 26 tasks, for 3 model scales. With 400 million training data points on ViT-B/32, MetaCLIP
outperforms CLIP by +2.1% on ImageNet and by +1.6% on average. On ViT-B/16, MetaCLIP
outperforms CLIP by +2.5% on ImageNet and by +1.5% on average. On ViT-L/14, MetaCLIP
outperforms CLIP by +0.7% on ImageNet and by +1.4% on average across the 26 tasks.
We next turn to Pool 2 which is a larger set of image-text pairs and study the effect of scaling data. In
Table 5, we scale data to 1B and 2.5B and observe a large gain over 400M, with similar performance
for both 1B and 2.5B scales. Note that the number of training iterations (and therefore compute)
is the same for all rows. The main difference between 1B and 2.5B is the threshold t, where 1B is
a more balanced set by adding more data points (compared to the 400M set) to tail entries (up to
t = 20k), instead the 2.5B set adds (up to t = 170k) data points to all, head and tail, entries. The
extra data in the tail entries (1B set), seems to benefit downstream accuracy for tasks on specific
data such as CUB fine-grained bird classification, Flowers, KITTI, PCAM, while the larger 2.5B
data that has more head entries increases broadly over more datasets, but each at a smaller amount.
The overall average accuracies are similar for 1B and 2.5B (e.g., 70.2% vs. 69.8% for ViT-L model
8

Preprint

Pool 2 (10.7B)

Pool 2, t=170k (2.5B)

Pool 1 (1.6B)

Pool 2, t=20k (1B)

Pool 1, t=20k (400M)

Figure 3: Cumulative sum of counts on entries from tail to head on a data Pool 2. We again show (1)
raw/unbalanced cumulative counts), t = ∞; (2) balanced cumulative counts after applying t = 20k
and t = 170k. t defines maximum number of counts per entry and the transition of tail/head entries.
We show the Pool 1 configuration from Fig. 2 as dashed lines for reference.
size). On ImageNet, the 2.5B training data achieves 67.6% on ViT-B/32 that breaks the previous
believed saturated B/32 models (Cherti et al., 2022), 79.2% on ViT-L/14 , 80.5% on ViT-H/14 and
82.1% on ViT-bigG/14.
We plot the cumulative sum of counts for entries sorted by counts from tail to head in Fig. 3 for
all these cases, similar to Fig. 2 for Pool 1 (and the Pool 1 configuration as dashed lines). The plot
shows that the 2.5B data is still relatively long-tail, while the 1B data is more balanced, explaining
it’s better performance on specific data such as bird and flower types observed above.
4.2

A BLATION S TUDY

We show ablations for MetaCLIP for the 400M scale and ViT-B/32 in Table 6. We first ablate
different balancing thresholds t. We observe that the choice of t = 20k by CLIP yields the best
performance for ImageNet and averaged accuracy and t = 15k or t = 35k are slightly worse.
To understand the key effect of balancing, we use the whole matched pool (1.6B image-text pairs) to
train CLIP. Surprisingly, training on 4× more data (on head entries) significantly hurts the accuracy
on ImageNet (61.9 vs 65.5) and averaged accuracy across 26 tasks (56.6 vs 58.2).

30.7
30.6
32.4
32.2
29.1

17.2
16.1
17.7
15.1
17.6

55.5
52.3
56.1
51.0
58.8

SST2

35.8
34.8
40.2
41.4
36.2

HatefulMemes

64.2
64.1
63.8
58.2
64.8

CLEVR

52.6
54.0
50.0
55.2
54.5

UCF101

95.4
96.5
96.2
95.1
96.5

Kinetics700

35.9
39.7
23.0
37.0
42.1

PCAM

41.5
35.8
42.0
58.2
38.3

Country211

68.5
69.5
68.0
64.5
68.8

GTSRB

FER-2013

91.9
91.9
91.8
91.5
92.7

KITTI

MNIST

88.7
88.8
88.5
83.1
88.2

RESISC45

Flowers

52.8
52.1
52.7
51.0
55.1

STL-10

Caltech-101

26.8
25.6
28.5
19.9
27.9

EuroSAT

Pets

Cars

70.7
69.4
72.0
77.0
71.6

Aircraft

SUN397

63.0
64.6
65.0
65.8
65.4

DTD

CUB

63.4
65.7
63.0
50.8
65.7

CIFAR10

65.5 80.6 91.3 70.2
65.5 79.9 90.4 68.8
65.4 79.3 91.2 69.0
61.9 76.9 90.0 67.6
66.1 80.8 89.9 68.8

Food-101

CIFAR100

MetaCLIP t=20k
58.2
- t=15k
57.5
- t=35k
57.8
- unbalanced (1.6B) 56.6
- online balancing 58.5

ImageNet

Average

Balancing can also be applied online in the data loader with head entries down-sampled leading to
slightly better performance (58.5 vs 58.2); see appendix for details. This is useful if head data has
already been collected and one wants to train on a different distribution. The better accuracy for
online balancing is explained by the larger diversity in head data.

66.1
67.1
64.2
59.2
66.0

45.4
45.4
44.8
42.6
45.8

30.6
22.3
28.0
17.2
22.0

56.4
51.2
55.4
55.6
56.0

53.4
53.8
54.2
52.6
52.4

Table 6: Ablation studies on balancing in MetaCLIP. Default: t=20k, 400M. Model: ViT-B/32.

5

C ONCLUSION

In this paper, we attempt to reveal CLIP’s data curation. Our MetaCLIP builds upon metadata for
curation and balancing of raw data sourced from the web. Curating with metadata and balancing
are essential for good data quality, significantly outperforming the use of raw data. Our experiments
show that MetaCLIP performs well for different scales sourced from CommonCrawl data and outperforms CLIP’s proprietary data source, without reliance on any external model. We make our
pipeline for generating the data publicly available.
9

Preprint

ACKNOWLEDGMENTS
We thank Zeyuan Allen-Zhu, and Chunting Zhou for the insightful discussion and Brighid Meredith
for suggestions on scaling the pipeline.

A PPENDIX

A.1

A DDITIONAL R ESULTS

Curation from DataComp-12.8B.
The concurrent work Gadre et al.
(2023) released a collection of 12.8B
image-text pairs from CommonCrawl
from 2014-2022. We further investigate whether we can apply the algorithm on its 12.8 unfiltered pool.
Although the unfiltered pool seemingly offers an opportunity to apply
our algorithm on a publicly available
source, our initial studies show that,
implicit biases may still be present
in this pool. For example, we notice that all image URLs are collected
as a string starting with http. This
excludes relative URLs that could
be frequently used by quality websites (with potentially good imagetext pairs). We curate from DataComp’s 12.8B unfiltered pool with
t=60k, which has 6% of tail counts
that is the same as t=20k for 400M
from our 1.6B pool.
When using 1B image-text pairs curated from DataComp’s pool we notice a quality drop during training,
compared to data curated from our
pools, see Fig. 4.
Our smaller
400M set is slightly better than using DataComp-1B and our larger sets
(1B, 2.5B) are significantly better.

0.70
MetaCLIP(2.5B)
MetaCLIP(1B)
MetaCLIP(400M)

0.65

ImageNet Zero-shot Acc.

A

MetaCLIP(1B,DataComp)

0.60

0.55

0.50

CLIP(400M)
LAION(407M)
MetaCLIP(1B,DataComp)
MetaCLIP(400M)
MetaCLIP(1B)
MetaCLIP(2.5B)

0.45

0

50000

100000 150000 200000 250000 300000 350000 400000

Training Steps

Figure 4: ViT-B/32 on our Pool 1, Pool 2 and DataComp’s
unfiltered 12.8B pool. We show ImageNet zero-shot classification with a fixed 12.8B seen pair training budget. MetaCLIP’s curation is effective for all pools. However, with the
same curation method, the unfiltered DataComp-12.8B pool
lacks quality (we suspect it is caused by implicit filters in
the parser of DataComp).

HatefulMemes

SST2

CLEVR

Kinetics700

UCF101

PCAM

Country211

GTSRB

KITTI

RESISC45

EuroSAT

STL-10

FER-2013

MNIST

Flowers

Caltech-101

Pets

Aircraft

DTD

Cars

CUB

SUN397

CIFAR100

CIFAR10

Food-101

Average

ViT-B/32
MetaCLIP (400M)
58.2
MetaCLIP (1B,DataComp) 57.5
ViT-B/16
MetaCLIP (400M)
61.1
MetaCLIP (1B,DataComp) 61.2
ViT-L/14
MetaCLIP (400M)
67.1
MetaCLIP (1B,DataComp) 66.3

ImageNet

In Table 7, we show our 400M data vs our curated DataComp-1B data at various model scales,
where the same observation holds, suggesting our raw data pool is more effective.

65.5
65.4

80.6 91.3 70.2 63.4 63.0 70.7 26.8 52.8 88.7 91.9 68.5 41.5 35.9 95.4 52.6 64.2 35.8 30.7 17.2 55.5 66.1
81.5 88.7 68.5 59.4 64.7 76.6 18.1 57.5 89.9 92.4 67.3 40.7 38.2 96.5 41.2 62.3 43.4 35.1 17.6 53.5 61.5

45.4 30.6 56.4 53.4
44.8 19.2 57.0 54.3

70.8
70.7

86.8 90.1
88.1 91.3

66.5 70.8 66.6 74.1 27.9 55.9 90.4 93.8 72.3 47.8 44.6 97.2 55.4 68.8 43.8 33.4 22.6 52.9 68.0 49.5 22.8 54.8 60.6
71.6 64.5 67.6 81.2 21.6 62.3 92.7 93.2 70.7 55.6 39.6 97.7 52.9 66.3 45.7 36.0 22.3 50.1 68.1 49.2 17.1 56.4 59.9

76.2
76.7

90.7 95.5
92.6 95.6

77.4 75.9 70.5 84.7 40.4 62.0 93.7 94.4 76.4
78.9 72.1 71.6 87.1 31.6 67.5 93.4 95.3 76.0

61.7 46.5 99.3 59.7 71.9 47.5 29.9 30.9 70.1 75.5 57.1 35.1 56.6 65.6
65.1 42.4 99.1 61.7 69.8 45.9 36.8 31.8 51.0 76.6 57.5 29.3 57.1 60.8

Table 7: MetaCLIP curating our 400M data vs curating 1B data from DataComp-12.8B: The pool
of DataComp leads to a quality drop with performance closer to our 400M set.

DataComp Benchmark We also evaluate MetaCLIP on the benchmark used by (Gadre et al.,
2023) that contains 38 tasks including variants of ImageNet, retrieval, VTAB, etc. For simplicity,
we average the scores over each category.
10

Preprint

Avg.
ViT-B/32
CLIP (400M)
51.5
OpenCLIP (407M) 52.7
MetaCLIP (400M) 53.5
MetaCLIP (1B)
54.2
MetaCLIP (2.5B) 55.4

IN

IN Dist. Shift VTAB Avg. Retrieval

63.4
62.9
65.5
67.3
67.6

48.2
48.5
50.4
51.9
52.3

50.5
53.0
54.1
53.6
55.3

48.0
50.7
50.6
51.1
52.6

ViT-B/16
CLIP (400M)
55.5
OpenCLIP (407M) 56.1
MetaCLIP (400M) 57.5
MetaCLIP (1B)
58.4
MetaCLIP (2.5B) 60.0

68.3
67.0
70.8
72.4
72.1

54.1
52.6
55.5
57.8
57.7

54.4
54.9
56.7
56.3
59.0

50.2
53.9
53.9
54.3
54.0

ViT-L/14
CLIP (400M)
61.4
OpenCLIP (407M) 59.7
MetaCLIP (400M) 62.2
MetaCLIP (1B)
65.0
MetaCLIP (2.5B) 65.6

75.5
72.7
76.2
79.0
79.2

61.6
57.3
61.3
64.5
64.6

59.5
58.6
59.8
62.5
64.1

53.6
55.9
57.3
58.3
60.1

ViT-H/14
MetaCLIP (2.5B)

66.5

80.5

66.1

64.6

60.4

ViT-bigG/14
MetaCLIP (2.5B)

68.3

82.1

67.6

66.5

62.6

Table 8: Zero-shot classification and retrieval on tasks from (Gadre et al., 2023).

Note that prompts and class names used by (Gadre et al., 2023) could be different from prompts and
classnames used by OpenAI CLIP.
From Table 8, we can see that MetaCLIP outperforms CLIP and OpenCLIP across various model
sizes. First, for the same data scale (400M), MetaCLIP outperforms OpenCLIP, which is better
than CLIP on this benchmark, by +1.4% for ViT-B/16 and +2.5% for ViT-L/14, when comparing
average accuracy across the 38 tasks. Second, for increasing our MetaCLIP data size to 1B we see a
significant gain, especially for the larger model, from 62.2% to 65.0% average accuracy. Using our
larger dataset with 2.5B and more head entries leads to a further gain to 65.5%.
We further detail the breakdown of each dataset in Table 9 (please view in landscape orentation).

11

63.3
62.9
65.6
67.3
67.7

68.3
67.0
70.8
72.4
72.1

75.6
72.8
76.2
79.0
79.2

80.5

82.1

ViT-B/16
CLIP (400M)
OpenCLIP (407M)
MetaCLIP (400M)
MetaCLIP (1B)
MetaCLIP (2.5B)

ViT-L/14
CLIP (400M)
OpenCLIP (407M)
MetaCLIP (400M)
MetaCLIP (1B)
MetaCLIP (2.5B)

ViT-H/14
MetaCLIP (2.5B)

ViT-bigG/14
MetaCLIP (2.5B)

ImageNet 1k

ViT-B/32
CLIP (400M)
OpenCLIP (407M)
MetaCLIP (400M)
MetaCLIP (1B)
MetaCLIP (2.5B)

ImageNet-O

ImageNet-A

ImageNet v2

ImageNet Sketch

12

72.8

70.5

59.6
59.6
65.0
68.9
68.9

48.3
52.4
57.9
60.5
60.2

42.3
49.4
53.3
55.3
56.0

76.0

74.1

69.8
65.4
69.8
72.5
72.6

61.9
59.7
62.6
65.1
65.0

56.0
55.1
57.6
59.2
59.6

42.3
50.6
39.2
41.9
41.5

47.8
53.5
46.8
48.2
48.3

78.4

75.4

28.9

30.2

70.7 32.3
46.5 41.9
66.4 28.9
70.4 31.6
72.3 30.1

49.9
33.2
47.0
49.2
49.5

31.5
21.7
28.6
29.7
29.9

ImageNet Distribution Shift

ImageNet-R

94.1

93.4

87.9
84.7
88.9
91.0
92.0

77.7
77.9
81.8
82.7
84.2

69.4
73.4
74.8
76.1
78.1

Caltech-101

95.7

95.3

92.5
92.6
94.6
58.4
95.3

89.0
91.3
93.4
93.5
93.3

87.6
91.2
91.7
93.7
92.9

CIFAR-100

88.3

86.4

75.8
77.4
77.3
84.6
84.1

66.9
71.2
66.5
77.9
78.9

64.2
70.3
70.0
76.5
77.7

CLEVR Counts

17.0

21.1

19.5
24.3
22.7
15.8
31.0

21.2
28.7
30.1
19.3
29.0

23.3
16.2
21.5
15.7
18.7

CLEVR Distance

Describable Textures

72.1

72.7

55.3
60.5
62.4
65.8
68.6

44.9
51.3
55.7
58.2
62.2

44.3
54.6
52.5
59.1
58.8

EuroSAT

72.8

64.5

62.6
62.3
60.4
68.3
59.0

56.0
50.2
55.7
47.6
52.7

50.5
51.5
52.4
48.0
49.9

KITTI Vehicle Distance

26.7

27.7

21.8
20.1
24.2
23.2
27.9

26.3
18.1
24.2
24.6
18.4

27.4
28.8
25.9
20.5
18.7

VTAB

Oxford Flowers-102

89.1

84.5

79.2
73.1
76.5
83.8
81.4

69.1
66.9
72.3
74.8
73.5

66.6
66.2
68.1
72.7
69.4

Oxford-IIIT Pet

96.2

95.6

93.2
91.7
93.8
95.2
94.6

88.9
89.2
90.4
92.1
91.7

87.0
86.7
88.8
89.5
90.9

RESISC45

74.0

70.3

63.3
67.4
68.5
68.0
73.6

58.2
58.5
66.1
65.1
67.4

53.6
54.5
59.6
61.6
61.2

SUN397

75.1

74.4

67.6
72.6
70.8
72.9
73.6

64.4
69.6
66.8
66.8
68.8

62.5
67.0
63.4
66.3
66.9

SVHN

60.4

62.8

52.8
49.5
32.4
38.0
46.8

40.2
34.1
25.2
27.8
39.1

14.9
30.4
20.5
21.7
34.3

Camelyon17

76.1

66.2

69.6
45.5
69.2
79.1
75.5

60.2
59.9
67.7
60.1
69.8

51.6
47.1
64.5
49.4
56.6

MSCOCO

Retrieval

WinoGAViL

Flickr

85.4

85.0

58.1

57.5

44.4

38.8

75.1 46.4 39.2
78.9 51.3 37.4
79.8 51.9 40.2
82.2 54.2 38.6
83.3 55.7 41.4

72.2 42.8 35.7
74.5 46.9 40.2
76.7 48.2 36.9
77.1 48.9 36.8
78.1 50.4 33.5

68.8 40.3 34.9
70.2 43.9 38.0
70.1 43.9 37.8
72.6 45.1 35.6
72.9 46.6 38.2

Country211

CIFAR-10

98.5

98.1

32.5
42.0
35.8
40.7
39.3

GTSRB

41.5 52.7

94.8

94.2

MNIST

76.3
76.1
61.8
53.9
64.4

51.3
66.2
47.8
36.3
66.1

48.3
37.5
41.4
35.6
42.6

ObjectNet

68.9
59.7
69.2
73.9
74.6

55.3
51.5
59.1
60.0
61.4

44.3
43.9
50.5
49.4
52.8

Pascal VOC 2007

78.3
75.6
74.4
80.9
80.3

78.3
76.8
72.2
79.2
78.2

76.4
75.8
70.8
79.0
76.5

PatchCamelyon

52.0
49.7
70.3
73.1
62.0

50.7
59.6
61.9
65.7
59.2

62.2
55.9
64.2
50.0
56.0

Rendered SST2

68.9
56.0
65.6
66.0
66.8

60.5
54.4
60.5
61.3
59.1

58.6
52.3
53.5
57.1
53.2

Stanford Cars

77.9
89.6
84.8
86.6
88.8

64.7
83.7
74.2
79.0
83.0

59.6
79.3
70.6
73.0
77.3

STL-10

99.4
98.1
99.3
99.3
99.3

98.3
97.0
97.2
98.0
98.4

97.1
95.6
95.4
96.3
96.3

iWildCam

12.3
12.5
14.1
16.2
15.8

11.1
10.3
11.2
13.2
12.3

6.7
7.4
8.2
8.4
9.2

FMoW

15.2
17.0
18.7
28.1
25.9

16.7
15.5
19.9
15.8
19.4

13.3
13.0
0.0
14.8
15.9

Dollar Street

63.0
61.7
67.3
69.4
67.4

58.8
59.3
60.6
63.3
64.0

53.9
54.9
59.7
58.9
60.5

88.4
88.4
89.3
91.1
91.4

86.1
85.3
88.9
87.9
88.7

82.2
83.8
85.4
86.0
86.1

GeoDE

61.7 78.1 77.2 74.7 75.8 65.3 90.8 99.5 18.3 18.5 70.2 92.5

62.6 72.7 76.5 75.0 62.3 69.1 89.9 99.4 17.3 15.6 68.1 90.7

31.7 93.1 50.6
24.9 90.1 49.9
39.8 90.7 47.5
49.6 93.1 58.6
45.3 93.5 56.3

24.3 88.7 43.3
17.7 86.1 43.4
28.4 87.2 43.7
38.1 88.3 49.5
30.5 88.8 46.5

19.5 84.0
16.6 80.9
26.9 81.0
31.0 82.6
27.1 83.1

FGVC Aircraft

37.2 51.0

95.6 31.9
94.6 23.0
95.5 30.9
96.8 32.2
97.6 33.9

90.8 22.8
91.7 18.1
90.1 22.6
94.8 21.6
95.7 22.7

89.8 17.2
90.7 14.7
91.3 17.1
95.2 16.9
95.2 17.7

Food-101

Table 9: Zero-shot evaluation of Table 8 broken down by datasets.

20.3

18.8

20.2
24.5
25.1
22.5
22.6

22.3
24.5
22.4
24.4
22.6

23.4
23.9
24.5
22.5
23.1

Preprint

Preprint

A.2

D ETAILS ON E FFICIENT C URATION

Curation in Data Pipeline. Our data collection pipeline contains 5 major components: HTML
parsing/Language-Identification, URLs & Text deduplication, Image Download, NSFW image filter
& deduplication and packaging, which are described next.
Our HTML Parser is applied to all WAT files of CommonCrawl by keeping all <img> tags with
both relative and absolute URLs and one or multiple text keys.
Language-Identification(LID): we use an internal language identification system that can detect 191
languages with different dialects and keep all texts that are tagged as English (or its dialects).
URLs / Text deduplication: We use 24bit sha224 hashing to encode an URL and into tables to
deduplicate columns with the same hash, avoiding downloading the same image URL multiple times.
URLs with illegal domains are removed; if there are multiple texts associated with the same image,
these are further deduplicated; texts with NSFW keywords are removed;
NSFW filter: We use an internal system that can classify inappropriate content in images into 96
types of dangerous content and discard such image/text pairs. Images are further deduplicated by
64-bit PCA hash, derived from a similarity search model’s feature embeddings with PCA reduction
to 64 dimensions and sign quantization.
Our curation algorithm does not require access to images, making it suitable for integration into a
pipeline to reduce the scale of data points after parsing and before image downloading. We designed
the algorithm to be modular, allowing different parts to be placed at different stages in the pipeline,
as shown in Figure 5.
Specifically, sub-string matching can be placed immediately after HTML parsing to reduce data
points for English-only pairs (e.g. by ∼50%). Balancing can be applied earlier, before image downloading, to further reduce data points by ∼77%. This approach led to a total reduction of ∼90%,
allowing for curation of the entire CommonCrawl data without the need to store and transfer all data
points, which allows us to curate the whole CommonCrawl since 2013 with 300B+ scale URL-text
pairs without spending the storage/transfer on all ∼10× data points, where the rate of keeping a data
point with MetaCLIP curation is ∼0.1 (0.5 × 0.23).
HTML
Parsing/LID

URLs & Text
Dedup

Image
Download

Part1:
Sub-string
Matching

Part2:
Balancing

50% reduction

77% reduction

NSFW
Image Dedup

Packaging

Figure 5: Case study: Curation implementation in our data pipeline.
Curation in Data Loading We applied the balancing/sampling part of the algorithm to the data
loader to adjust data distribution on-the-fly. Although data points for tail entries are always sampled
in Algorithm 1, the diverse pairs from the head entries are sub-sampled from a larger pool while
maintaining a similar distribution as offline curation. This diversification of pairs that match the
head entries improved the performance, as shown in Table 6.
A.3

H UMAN S TUDY ON THE E FFECTS OF C URATION

In this section, we study the impact of MetaCLIP curation on data distribution by using human
evaluation. We approach this exploration from three distinct angles: noise reduction, alignment of
visual content, and task-agnostic attributes. In the pursuit of comprehending the first two aspects,
we undertake a human study aimed at comprehending the data quality implications of implementing
the balancing technique (outlined in Part 2 of Algorithm 1). This evaluation encompasses three
dimensions: image-only, text-only, and image-text alignment. We collect an evaluation set of 100
random image-text pairs for balanced and unbalanced data, respectively, and ask annotators to score
on the image, text, and pair quality metrics, separately, on a scale of 1 to 5.
13

Preprint

Annotation Guidelines. Annotators follow guidelines to assess both images and texts, evaluating
informativeness (how well information is conveyed) and aesthetics. For images, aesthetics considers
visual elements like composition, color, lighting, balance, contrast, texture, and subject matter. For
texts, aesthetics gauges factors like delimiters, sentence structure, capitalization, prefixes/suffixes,
recognized words, generic words, and overall text quality. The alignment metric for image-text pairs
measures the relevance between the two modalities, assessing how well the text describes the image
content. Ratings are averaged across annotators for each dimension.
We show the study results in Table 10 and discuss the different criteria next.
Evaluation Dimension

Rating for Balanced Data

Rating for Unbalanced Data

P-value

Image

4.60, [4.50, 4.70]

4.36, [4.23, 4.48]

< 0.001

Text

4.67, [4.56, 4.78]

4.06, [3.82, 4.30]

< 0.001

Alignment

4.41, [4.23, 4.59]

3.72, [3.46, 3.99]

< 0.001

Table 10: Average human rating on the effect of balancing on data quality, with confidence intervals
shown in parentheses. Higher rating is better. Balanced data is rated of higher quality.
Noise Mitigation in Image and Text. As shown in Table 10, significant quality improvement for
all the three evaluation dimensions is observed after applying balancing. MetaCLIP curation has
no specific hard filters such as removing shorter text, removing dates, etc. However, curation by
sub-string matching and balancing has a different filtering effect. For example, a sub-string itself
can never curate a date-only text. Further, balancing allows signal and noise to co-exist when they
are difficult to be separated by human designed filters. For example, if one entry such as “image” or
“photo” is capped to t = 20k, it can only contribute 0.005% of 400M data.
Visual Content Alignment. Although MetaCLIP curation does not directly involve images, it has
a positive effect on aligning visual content by controlling the quality and distribution of text. First,
sub-string matching increases the chance of having (visual) entities mentioned in the text, thereby
improving the likelihood of finding corresponding visual content. Second, balancing favors longtailed entries that could have more diverse visual content than a head entry (such as the text “1”).
In Table 10, we observe significant improvement on pair quality from unbalanced data to balanced
data.
A.4

M EASURING TASK -A LIGNMENT

HatefulMemes

SST2

CLEVR

Kinetics700

UCF101

PCAM

Country211

GTSRB

KITTI

RESISC45

EuroSAT

FER-2013

STL-10

Flowers

MNIST

Caltech-101

Pets

Aircraft

DTD

Cars

CUB

SUN397

CIFAR10

CIFAR100

Food-101

ImageNet

With metadata, one can measure the alignment of pre-training data distribution with the data distribution in downstream tasks. First we can do a simple measure of counting the number of metadata
substring matches that are directly corresponding to class names in the downstream tasks. In the
first row of Table 11, we show the accuracy of MetaCLIP (400M), ViT-L (cf. Table4) for reference.
The second row shows the number of classes that are present in the metadata, for each downstream
dataset. For example, for ImageNet 703 of the 998 unique class names are present in the metadata.
Interestingly, there seems to be a correlation with the accuracy and the number of classes matched
in the metadata.

MetaCLIP (400M) ViT-L
76.2
90.7 95.5 77.4 75.9 70.5 84.7 40.4 62.0 93.7 94.4 76.4 61.7 46.5 99.3 59.7 71.9 47.5 29.9 30.9 70.1 75.5 57.1 35.1 56.6 65.6
# of cls. w/ non-zero counts 703/998 52/101 10/10 93/100 1/200 193/397 0/196 8/100 40/47 15/37 86/102 61/102 10/10 12/12 10/10 2/10 32/45 1/43 0/4 190/211 1/2 5/101 122/700 8/8 1/2 2/2
KL-divergence
- unbal.
6.8
9.4
7.5
6.1 11.4
6.7
0.0 12.0 9.7 11.7 7.7
10.6 3.4 8.9 7.1 8.4 7.6 9.3 0.0
5.7 14.6 8.9
8.9
3.8 9.2 9.8
- bal.
5.1
7.4
8.1
6.0 10.4
6.0
0.0 10.1 8.0 9.7
7.1
8.6
8.1 8.3 8.1 9.7 7.2 10.4 0.0
5.2 12.5 8.8
7.1
8.3 10.4 9.7

Table 11: Measuring task-alignment. First row: MetaCLIP (400M) ViT-L/14 accuracy, second row:
number of classes matched in metadata, Third and fourth row: We use KL-divergence (lower is
better) to measure the similarity between pre-training distribution and benchmark task distribution.
Note that, for ImageNet, two classes have duplicated class names and therefore there are only 998
unique classes (out of 1000).

14

Preprint

Hyperparameter
OpenAI CLIP / MetaCLIP
OpenCLIP
DataComp
Activation Function
QuickGELU
GELU
GELU
Seen Pairs
12.8B(400M×32 epochs)
13B (407M×32 epochs)
12.8B
Batch Size
32768
32768 (B/32), 33792 (B/16), 38400 (L/14) 90112 (L/14)
Learning Rate
5.0e-4(B/32,B/16), 4.0e-4(L/14)
5.0e-4(B/32)
1e-3(L/14)
Warm-up
2k
2k (B/32)
10k (L/14)

Table 12: Hyperparameters of OpenAI CLIP vs OpenCLIP on LAION-400M(Schuhmann et al.,
2021) and DataComp 1B.
Next, we use KL-divergence to measure alignment:
X
P (m) 
DKL (T ||P ) = −
T (m) log
,
T (m)

(1)

m∈M

where T (m) represents the task distribution over M and P (m) represents the pre-training data
distribution. We compute T (m) using the benchmark data distribution over class labels of a task.
Taking ImageNet as an example from Table 11, T (m) ImageNet has 998 entries uniformly distributed (each has evenly 50 examples) and P (m) has normalized counts over all 500k entries but
only 703 entries used to compute KL-divergence. In the third and fourth row of Table 11, we can
see improvements for balanced data points over unbalanced ones for each task, showing balancing
improves similarity with most tasks.
A.5

T RAINING S ETUP OF O PENAI CLIP VS O PEN CLIP

Our work strictly follows CLIP’s setup for a controlled comparison focusing on data curation and
quality. We notice differences in the training setup of OpenCLIP5 and list the difference (known to
us). OpenCLIP varies the setup from CLIP (e.g., global batch size, learning schedule, etc.). Here
we only list the difference for LAION-400M(Schuhmann et al., 2021), which is closer to the CLIP
setup. We note that DataComp differs even more, e.g., by curating images close to ImageNet training
data, a large batch size of 90k that is almost 3× larger than CLIP, and using the CLIP model to filter
data.
A.6

B ENCHMARK D EDUPLICATION

Our pools are deduplicated from the benchmark/ImageNet data using a 64-bit PCA hash, derived
from a similarity search model’s feature embeddings with PCA reduction to 64 dimensions and sign
quantization. DataComp-12.8B is already deduplicated.
A.7

N EGATIVE R ESULTS L EARNED FROM A BLATING CLIP C URATION

We briefly describe a few ideas close to CLIP curation that did not look promising in our initial
attempts and were abandoned:
1. Self-curated Metadata. We initially attempted to build metadata directly from the text
in raw image-text pairs (i.e., using terms appearing in text above a certain threshold of
counts). We rank entries by count and keep the top 500,000. Metadata built this way
appeared worse. We notice that although the top frequent entries are similar to CLIP’s
metadata, the long-tailed part is very different. For example, the minimal count to be in the
500,000 budget needs at least 130 counts. In contrast, our metadata has 114K entries that
have no matches. This approach results in worse quality metadata including low-quality
spelling/writing (instead of high-quality entries from WordNet or Wikipedia). Further, the
effect of balancing saturates earlier for such data (in a larger t, verified by CLIP training)
since low-quality entries are also heavily in long-tail.
2. Cased WordNet. We also notice many cased words are missing from metadata (e.g., WordNet is in lowercase). After adding cased WordNet into metadata, we notice a performance
drop on ImageNet. The reason could be class names are more likely in lower case and
upper case entry matching may reduce the written quality of texts.
5

https://github.com/mlfoundations/open_clip

15

Preprint

3. Stopwords/Useless Entries Removal We further study the effect of whether removing
stopwords and useless words such as “photo” and “image” is beneficial. This led to almost no difference since balancing entries reduced the effects of useless entries (each entry
contributes to 0.0002% (1/500k) level of the total data points). To encourage a simplified
solution, we do not intend to add more artificial filters.
A.8

M ORE D ETAILS ON D ISTRIBUTION ON M ETA DATA

Extending the top-20 matches in Table 3, we further group counts of metadata entries and show 5
examples per group as in Table 13.
Group
0-10k
10k-20k
20k-50k
50k-100k
100k-500k
500k-1M
1M-50M
50M-130M

5 Examples (Entry:Count)
ivailo:12, Kunta Kinte:201, vikernes:33, peria:50, ankoku:20
queer:19k, barry:10k, bandages:12k, The Police:15k, sigma:14k
polygonal:21k, widely:28k, however:35k, toppers:25k, executives:21k
planted:52k, olive oil:58k, yours:63k, packages:82k, Spokane:53k
compact:133k, vertical:222k, underwear:111k, powder:323k, weekly:130k
Tokyo:713k, Lead:620k, Diagram:809k, Dye:858k, unnamed:512k
see:1.4M, Door:3.2M, News:2.3M, sea:1.1M, street:1M
with:67M, and:100M, to:61M, in:107M, of:121M

Table 13: Distribution of metadata entries with counts, similar as head shown in Table 3.
A.9

R ANDOMNESS OF A LGORITHM 1

We further study the randomness of algorithm 1 by running it 3 times for Pool 1(400M) and Pool
2(2.5B). This ended with a standard deviation of 4035 examples for Pool 1 and 2104 examples for
Pool 2, respectively.
A.10

Q UALITATIVE DATA E XAMPLES

In Table 14, we illustrate data before/after sub-string matching and balancing. We also highlight
class labels from ImageNet in the table. We mark a matched entry with a bigger font size indicating
higher probability of sampling that entry. Intuitively, sub-string matching removes low quality text
and balancing favors longer text with long-tail entities to improve data quality. In Table 15, we show
more examples of matched text that include ImageNet tail entries.

16

Preprint

Text
control_14ct
cudd2008
product-img
Skirmisher-Main-440x412
A4omote
How-to-Find-the-Finest-Electric-Car-Companies
hanukkah-party-facebook-event-cover-template
johnny_cash_chili_dog (2)
8533 GOLDEN RIDGE COURT

How to build a stone patio on your own
battery plate

barn wedding
Picture sand , machine , Concept ,
front , Slim , Wrangler , Jeep

jeep , the concept , the

desk
Adult T-shirt
Imix m10 stage moniter
google wallet md3
Distressed beach bar sign - Pearly’s Oyster Bar
Why the Kilimanjaro Trek should be top of your bucket list
J70 desk model
Whitby castle
Inside the castle
Hama Hama Oyster Saloon | restaurant | 35846 US-101 , Lilliwaup , WA

Substr. Bal. IN Head IN Tail
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✓
✗
✓
✗
✓

✗

✓

✗

✓

✗

✓

✗

✓

✗

✓

✗

✓

✗

✓

✗

✓
✓
✓

✗
✗
✗

✓
✓
✓

✗
✗
✗

✓
✓
✓
✓
✓
✓

✗
✗
✗
✗
✗
✗

✓
✓
✓
✓
✓
✓

✗
✗
✗
✗
✗
✗

✓

✗

✓

✗

✓

✓

✓

✗

✓

✓

✓

✗

✓

✓

✓

✗

✓

✓

✓

✗

✓

✓

✓

✗

98555 , USA | 3608775811 OR +1 360-877-5811

beach

onions , sauteed red bell peppers and
zucchini combined create a winning egg frit-

Caramelized

tata breakfast dish .

layered paper cut craft style music composition
of saxophone guitar trumpet violin music instruments , notes on abstract color background . Jazz
Vector

concert festival party poster banner card template

Nautilus hot tub
night binoculars for hunting
2017 cat eyes women’s sunglasses for women vintage
sun glasses round women sun glasses oculos oculos de sol

feminino

Table 14: Examples categorized by whether passing sub-string matching and balancing: Words in
violet color are in metadata and their font size indicates probability of being sampled, ranging from

13pt that has probability close to 0, to
entries are cyan.

22pt that has probability 1. ImageNet labels in head
17

Preprint

Text

Substr. Bal. IN Head IN Tail

julep goblet , ✓

✓

✗

✓

✓

✓

✗

✓

✓

✓

✗

✓

✓

✓

✗

✓

✓

✓

✗

✓

✓

✗

✓

✓

✗

✓

✓

✗

✓

✓

✗

✓

✓
✓

✗
✗

✓
✓

Staffordshire Bull Terrier LED ✓

✓

✗

✓

✓

✓

✗

✓

✓

✗

✓

✓

✗

✓

✓

✗

✓

✓

✗

✓

Antique German sterling silver 800 cup

1

train

A journey to the East Sea by high-speed
: New KTX line makes Gangneung’s wonders all the more

accessible

little arrow design co watercolor rainbow blush shower
and mat

curtain

photo of antique silver top break

revolver

trombone model
✓
Staffordshire Bull Terrier
KI Plantation Timbers fire truck on the night of Jan- ✓
uary 3 , 2020 . Photography : Tim Wilson

✓

buffalo bath
✓
Basset Hound pup with Lionhead x Lop rabbit
Single serving of peach trifle
✓
✓
A tarantula (hairy arachnid belonging
water

to the Theraphosidae family of spiders) in a box , standing still
. Close-up shot .
Modern
Night Light Animal Pet Dog Puppy 3D Optical
Lamp Home Decor Table Lamp Desk Light

illusion

chair , CANVA , camping chairs
dispalying kukri machete fitted inside scab- ✓
bard
✓
jacksons chameleon
✓
Wall Mural - Insects pollinating blooming
rapeseed crops in field
✓
Scottish Terrier Phone Pocket
Amazon , rocking

Table 15: Examples passing both sub-string matching and balancing wit ImageNet tail classes:
Words in violet color are in metadata and their font size indicates probability of being sampled,
ranging from 13pt that has probability close to 0, to
labels in tail entries are in cyan.

18

22pt that has probability 1. ImageNet

Preprint

C ONTRIBUTIONS
Hu Xu: project lead, overall direction, data pipeline (parsing, dedup, download and mitigation),
training, efficiency optimization, evaluation and writing.
Saining Xie: advising on directions, impact, writing and define key terms.
Xiaoqing Ellen Tan: data ablation, tuning mitigation and performance tradeoff.
Po-Yao Huang: DataComp benchmark evaluation, porting code to new cluster.
Russell Howes: advising on adjusting data pipeline, suggestions on scaling of URLs deduplication.
Vasu Sharma: running face blurring on 2.5B collection.
Shang-Wen Li: team management, overall suggestion on direction, data collection for 2 rounds,
data mitigation, legal communication, data transfer, GPU allocation, coordination and public communication.
Gargi Ghosh: team management, GPU allocation.
Luke Zettlemoyer: general advising on direction and writing.
Christoph Feichtenhofer: advising on experimental setups, multi-round writing and editing.

R EFERENCES
Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Dataefficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540,
2023.
Olivier Bachem, Mario Lucic, and Andreas Krause. Coresets for nonparametric estimation-the case
of dp-means. In International Conference on Machine Learning, pp. 209–217. PMLR, 2015.
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for
contrastive language-image learning. arXiv preprint arXiv:2212.07143, 2022.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.
Dan Feldman, Matthew Faulkner, and Andreas Krause. Scalable training of mixture models via
coresets. Advances in neural information processing systems, 24, 2011.
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao
Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen
Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh,
Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In
search of the next generation of multimodal datasets, 2023.
Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering. In
Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pp. 291–300,
2004.
Mon-Fong Jiang, Shian-Shyong Tseng, and Chih-Ming Su. Two-phase clustering process for outliers detection. Pattern recognition letters, 22(6-7):691–700, 2001.
Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of
machine learning models. In International Conference on Machine Learning, pp. 6950–6960.
PMLR, 2020.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning, pp.
8748–8763. PMLR, 2021.
19

Preprint

Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,
Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of
clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.
Christoph Schuhmann, Romain Beaumont, Cade W Gordon, Ross Wightman, Theo Coombes,
Aarush Katta, Clayton Mullis, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson,
et al. Laion-5b: An open large-scale dataset for training next generation image-text models. 2022.
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information
Processing Systems, 35:19523–19536, 2022.
Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio,
and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network
learning. arXiv preprint arXiv:1812.05159, 2018.
Hu Xu, Saining Xie, Po-Yao Huang, Licheng Yu, Russell Howes, Gargi Ghosh, Luke Zettlemoyer,
and Christoph Feichtenhofer. Cit: Curation in training for effective vision-language data. arXiv
preprint arXiv:2301.02241, 2023.
Dantong Yu, Gholamhosein Sheikholeslami, and Aidong Zhang. Findout: Finding outliers in very
large datasets. Knowledge and information Systems, 4:387–412, 2002.

20

